[{"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 16, "a_bits": 16, "loss": 3.0677576065063477, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "NoQuantizer", "compile": false, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-NoQuantizer@1:NoQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 2, "a_bits": 2, "loss": 3.677242994308472, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-TrustQuantizer@2:TrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 3, "a_bits": 3, "loss": 3.425989151000977, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-TrustQuantizer@3:TrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 2, "a_bits": 2, "loss": 3.558492422103882, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-TrustQuantizer@2:TrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 3, "a_bits": 3, "loss": 3.282531499862671, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-TrustQuantizer@3:TrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 2, "a_bits": 2, "loss": 3.3408315181732178, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-TrustQuantizer@2:TrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 3, "a_bits": 3, "loss": 3.0895519256591797, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-TrustQuantizer@3:TrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 4, "loss": 3.3036246299743652, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 2, "a_bits": 2, "loss": 3.140669822692871, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-TrustQuantizer@2:TrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 4, "loss": 3.1660852432250977, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 3, "a_bits": 3, "loss": 2.906904697418213, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-TrustQuantizer@3:TrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 4, "loss": 2.9780874252319336, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 4, "loss": 2.809302568435669, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 8, "a_bits": 8, "loss": 3.221074342727661, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-TrustQuantizer@8:TrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 2, "a_bits": 2, "loss": 3.5740408897399902, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HadamardTrustQuantizer@2:HadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 4, "loss": 3.2721152305603027, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 8, "a_bits": 8, "loss": 3.0810747146606445, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-TrustQuantizer@8:TrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 4, "loss": 3.1351213455200195, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 2, "a_bits": 2, "loss": 3.4409945011138916, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HadamardTrustQuantizer@2:HadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 3, "a_bits": 3, "loss": 3.226297378540039, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HadamardTrustQuantizer@3:HadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 8, "a_bits": 8, "loss": 2.904025077819824, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-TrustQuantizer@8:TrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 3, "a_bits": 3, "loss": 3.036802291870117, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HadamardTrustQuantizer@3:HadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 2, "a_bits": 2, "loss": 3.2354586124420166, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HadamardTrustQuantizer@2:HadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 4, "loss": 2.9476780891418457, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 8, "a_bits": 8, "loss": 2.7433011531829834, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-TrustQuantizer@8:TrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 2, "a_bits": 2, "loss": 3.045738935470581, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HadamardTrustQuantizer@2:HadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 3, "a_bits": 3, "loss": 2.8613274097442627, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HadamardTrustQuantizer@3:HadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 4, "loss": 2.7817554473876953, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 4, "loss": 3.2369637489318848, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HadamardFourEightTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HadamardFourEightTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 4, "loss": 3.0519893169403076, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HadamardFourEightTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HadamardFourEightTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 4, "loss": 2.8907320499420166, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HadamardFourEightTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HadamardFourEightTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 16, "a_bits": 16, "loss": 2.885958194732666, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-NoQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 16, "a_bits": 16, "loss": 2.7287867069244385, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-NoQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 3, "a_bits": 3, "loss": 3.371907949447632, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HadamardTrustQuantizer", "compile": false, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HadamardTrustQuantizer@3:HadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 16, "a_bits": 16, "loss": 2.570779323577881, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-NoQuantizer@16:NoQuantizer@16-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 3, "a_bits": 3, "loss": 2.719254970550537, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-TrustQuantizer@3:TrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 3, "a_bits": 3, "loss": 2.6784238815307617, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-HadamardTrustQuantizer@3:HadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 4, "a_bits": 4, "loss": 2.712613582611084, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "HadamardFourEightTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "HadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-HadamardFourEightTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 4, "a_bits": 4, "loss": 2.6375515460968018, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 1, "a_bits": 1, "loss": 3.9874863624572754, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "ClipQuantizer", "a_quant": "ClipQuantizer", "trust": null, "p": null, "clip_scale": 1.25, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "ClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "ClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-ClipQuantizer@1:ClipQuantizer@1-c4-CLIP_SCALE-1.25", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 1, "a_bits": 1, "loss": 3.827122926712036, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "ClipQuantizer", "a_quant": "ClipQuantizer", "trust": null, "p": null, "clip_scale": 1.25, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "ClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "ClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-ClipQuantizer@1:ClipQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 1, "a_bits": 1, "loss": 3.6512818336486816, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "ClipQuantizer", "a_quant": "ClipQuantizer", "trust": null, "p": null, "clip_scale": 1.25, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "ClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "ClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-ClipQuantizer@1:ClipQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 800, "non_embedding_parameters": 822151168, "w_bits": 4, "a_bits": 4, "loss": 2.517138719558716, "tokens": 79999795200, "bops": 65771925063440793600, "w_quant": "TrustQuantizer", "a_quant": "TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 7.5e-05, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 2048, "n_head": 16, "a_quant": "TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 16, "w_quant": "TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 32, "decay_type": "linear", "from_dense": false, "iterations": 305175, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 30517, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-800M-TrustQuantizer@4:TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 4, "a_bits": 4, "loss": 2.613788604736328, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 1, "a_bits": 1, "loss": 3.9449031352996826, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-c4-CLIP_SCALE-1.3", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 1, "a_bits": 1, "loss": 3.4947926998138428, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "ClipQuantizer", "a_quant": "ClipQuantizer", "trust": null, "p": null, "clip_scale": 1.25, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "ClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "ClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-ClipQuantizer@1:ClipQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 1, "a_bits": 1, "loss": 3.790695905685425, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 1, "a_bits": 1, "loss": 3.601443290710449, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 1, "a_bits": 1, "loss": 3.4229445457458496, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 1, "a_bits": 1, "loss": 3.3117942810058594, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "ClipQuantizer", "a_quant": "ClipQuantizer", "trust": null, "p": null, "clip_scale": 1.25, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "ClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "ClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.25}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-ClipQuantizer@1:ClipQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 430, "non_embedding_parameters": 443068288, "w_bits": 1, "a_bits": 1, "loss": 3.2240843772888184, "tokens": 42999742464, "bops": 19051822277965381632, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.00015, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1664, "n_head": 13, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 13, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 164031, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 16403, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-430M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 800, "non_embedding_parameters": 822151168, "w_bits": 16, "a_bits": 16, "loss": 2.460976839065552, "tokens": 79999795200, "bops": 65771925063440793600, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 7.5e-05, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 2048, "n_head": 16, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 16, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 32, "decay_type": "linear", "from_dense": false, "iterations": 305175, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 30517, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-800M-NoQuantizer@16:NoQuantizer@16-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 800, "non_embedding_parameters": 822151168, "w_bits": 4, "a_bits": 4, "loss": 2.4949800968170166, "tokens": 79999795200, "bops": 65771925063440793600, "w_quant": "HadamardTrustQuantizer", "a_quant": "HadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 7.5e-05, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 2048, "n_head": 16, "a_quant": "HadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 16, "w_quant": "HadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 32, "decay_type": "linear", "from_dense": false, "iterations": 305175, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 30517, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-800M-HadamardTrustQuantizer@4:HadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 10000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 800, "non_embedding_parameters": 822151168, "w_bits": 1, "a_bits": 1, "loss": 3.0793442726135254, "tokens": 79999795200, "bops": 65771925063440793600, "w_quant": "HadamardClipQuantizer", "a_quant": "HadamardClipQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 7.5e-05, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 2048, "n_head": 16, "a_quant": "HadamardClipQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 16, "w_quant": "HadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 32, "decay_type": "linear", "from_dense": false, "iterations": 305175, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 30517, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-800M-HadamardClipQuantizer@1:HadamardClipQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 10000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 1, "a_bits": 16, "loss": 3.572735548019409, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardClipQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardClipQuantizer@1:NoQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 1, "a_bits": 16, "loss": 3.416841745376587, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardClipQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardClipQuantizer@1:NoQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 1, "a_bits": 16, "loss": 3.2309272289276123, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardClipQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardClipQuantizer@1:NoQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 1, "a_bits": 16, "loss": 3.064699649810791, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HalfHadamardClipQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": 1.3, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardClipQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 1, "clip_scale": 1.3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HalfHadamardClipQuantizer@1:NoQuantizer@1-OPTCLIP-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 2, "a_bits": 16, "loss": 3.372986078262329, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardTrustQuantizer@2:NoQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 2, "a_bits": 16, "loss": 3.228517532348633, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardTrustQuantizer@2:NoQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 2, "a_bits": 16, "loss": 3.0358951091766357, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardTrustQuantizer@2:NoQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 3, "a_bits": 16, "loss": 3.295581102371216, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardTrustQuantizer@3:NoQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 3, "a_bits": 16, "loss": 3.1490747928619385, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardTrustQuantizer@3:NoQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 3, "a_bits": 16, "loss": 2.955838918685913, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardTrustQuantizer@3:NoQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 16, "loss": 3.2472081184387207, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardTrustQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 4, "loss": 3.3790714740753174, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardFourEightTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardFourEightTrustQuantizer@:HalfHadamardTrustQuantizer@-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 16, "loss": 3.104909658432007, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardTrustQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 16, "loss": 2.915486097335815, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardTrustQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 3, "a_bits": 16, "loss": 2.7919774055480957, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HalfHadamardTrustQuantizer@3:NoQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 16, "loss": 2.755692481994629, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HalfHadamardTrustQuantizer@4:NoQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 800, "non_embedding_parameters": 822151168, "w_bits": 4, "a_bits": 4, "loss": 2.589355945587158, "tokens": 79999795200, "bops": 65771925063440793600, "w_quant": "HalfHadamardFourEightTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 7.5e-05, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 2048, "n_head": 16, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 16, "w_quant": "HalfHadamardFourEightTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 32, "decay_type": "linear", "from_dense": false, "iterations": 305175, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 30517, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-800M-HalfHadamardFourEightTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 10000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 2, "a_bits": 16, "loss": 2.865924596786499, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HalfHadamardTrustQuantizer@2:NoQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 4, "loss": 3.3523614406585693, "tokens": 1499987968, "bops": 45723473233838080, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 5722, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 572, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 2, "a_bits": 2, "loss": 3.6291232109069824, "tokens": 1499987968, "bops": 45723473233838080, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 5722, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 572, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 3, "a_bits": 3, "loss": 3.4351677894592285, "tokens": 1499987968, "bops": 45723473233838080, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 5722, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 572, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 3, "a_bits": 3, "loss": 3.5377707481384277, "tokens": 749993984, "bops": 22861736616919040, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 2861, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 286, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 4, "a_bits": 4, "loss": 3.4617700576782227, "tokens": 749993984, "bops": 22861736616919040, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 2861, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 286, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 2, "a_bits": 2, "loss": 3.718682765960694, "tokens": 749993984, "bops": 22861736616919040, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 2861, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 286, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-30M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 3, "a_bits": 3, "loss": 3.363795518875122, "tokens": 1249902592, "bops": 61941092777459712, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 4768, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 476, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 2, "a_bits": 2, "loss": 3.5417728424072266, "tokens": 1249902592, "bops": 61941092777459712, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 4768, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 476, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 4, "loss": 3.291682481765747, "tokens": 1249902592, "bops": 61941092777459712, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 4768, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 476, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 2, "a_bits": 2, "loss": 3.4779763221740723, "tokens": 2499805184, "bops": 123882185554919424, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 3, "a_bits": 3, "loss": 3.282963514328003, "tokens": 2499805184, "bops": 123882185554919424, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 4, "a_bits": 4, "loss": 3.2042269706726074, "tokens": 2499805184, "bops": 123882185554919424, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-50M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 3, "a_bits": 3, "loss": 3.1656694412231445, "tokens": 2499805184, "bops": 256924617229205504, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 2, "a_bits": 2, "loss": 3.328219175338745, "tokens": 2499805184, "bops": 256924617229205504, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 4, "loss": 3.0965049266815186, "tokens": 2499805184, "bops": 256924617229205504, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 9536, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 953, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 3, "a_bits": 3, "loss": 3.0892155170440674, "tokens": 4999872512, "bops": 513876177056694272, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 2, "a_bits": 2, "loss": 3.271866798400879, "tokens": 4999872512, "bops": 513876177056694272, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 4, "a_bits": 4, "loss": 3.01094651222229, "tokens": 4999872512, "bops": 513876177056694272, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-100M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 3, "a_bits": 3, "loss": 2.989391803741455, "tokens": 4999872512, "bops": 1015916495907061760, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 2, "a_bits": 2, "loss": 3.144622325897217, "tokens": 4999872512, "bops": 1015916495907061760, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 4, "loss": 2.9251291751861572, "tokens": 4999872512, "bops": 1015916495907061760, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 2, "a_bits": 2, "loss": 3.085181713104248, "tokens": 9999745024, "bops": 2031832991814123520, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 2}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 2}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@2:HalfHadamardTrustQuantizer@2-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 3, "a_bits": 3, "loss": 2.914547920227051, "tokens": 9999745024, "bops": 2031832991814123520, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 3}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 3}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@3:HalfHadamardTrustQuantizer@3-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 4, "a_bits": 4, "loss": 2.8412797451019287, "tokens": 9999745024, "bops": 2031832991814123520, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 2, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 4, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 4}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 4}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "HALF-200M-HalfHadamardTrustQuantizer@4:HalfHadamardTrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 16, "a_bits": 16, "loss": 3.2071399688720703, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "NoQuantizer", "a_quant": "NoQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "NoQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "NoQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "QUAD-30M-NoQuantizer@16:NoQuantizer@16-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 20000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 8, "a_bits": 8, "loss": 3.208911895751953, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardTrustQuantizer@8:HalfHadamardTrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 8, "a_bits": 8, "loss": 3.068520784378052, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardTrustQuantizer@8:HalfHadamardTrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 8, "a_bits": 8, "loss": 2.887951374053955, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardTrustQuantizer", "a_quant": "HalfHadamardTrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardTrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardTrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "a_quant_kwargs": {"bits": 8}, "parallel_block": false, "use_pretrained": "none", "w_quant_kwargs": {"bits": 8}, "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardTrustQuantizer@8:HalfHadamardTrustQuantizer@8-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 0, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 30, "non_embedding_parameters": 30482560, "w_bits": 16, "a_bits": 16, "loss": 3.2958412170410156, "tokens": 2999975936, "bops": 91446946467676160, "w_quant": "HalfHadamardFP4TrustQuantizer", "a_quant": "HalfHadamardFP4TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 640, "n_head": 5, "a_quant": "HalfHadamardFP4TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 6, "w_quant": "HalfHadamardFP4TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 11444, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1144, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-30M-HalfHadamardFP4TrustQuantizer@4:HalfHadamardFP4TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 1000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 50, "non_embedding_parameters": 49556736, "w_bits": 16, "a_bits": 16, "loss": 3.156088590621948, "tokens": 4999872512, "bops": 247777362110840832, "w_quant": "HalfHadamardFP4TrustQuantizer", "a_quant": "HalfHadamardFP4TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0012, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 768, "n_head": 6, "a_quant": "HalfHadamardFP4TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 7, "w_quant": "HalfHadamardFP4TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 19073, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 1907, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-50M-HalfHadamardFP4TrustQuantizer@4:HalfHadamardFP4TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 1000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 100, "non_embedding_parameters": 102777856, "w_bits": 16, "a_bits": 16, "loss": 2.9659204483032227, "tokens": 9999745024, "bops": 1027752354113388544, "w_quant": "HalfHadamardFP4TrustQuantizer", "a_quant": "HalfHadamardFP4TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0006, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1024, "n_head": 8, "a_quant": "HalfHadamardFP4TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 8, "w_quant": "HalfHadamardFP4TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 38146, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 3814, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-100M-HalfHadamardFP4TrustQuantizer@4:HalfHadamardFP4TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 1000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}, {"model_size": 200, "non_embedding_parameters": 203188480, "w_bits": 16, "a_bits": 16, "loss": 2.7984418869018555, "tokens": 19999752192, "bops": 4063719248269148160, "w_quant": "HalfHadamardFP4TrustQuantizer", "a_quant": "HalfHadamardFP4TrustQuantizer", "trust": null, "p": null, "clip_scale": null, "args": {"lr": 0.0003, "opt": "adamw", "bias": false, "seed": 0, "beta1": 0.9, "beta2": 0.95, "dtype": "bfloat16", "model": "llama", "wandb": true, "device": "cuda:0", "n_embd": 1280, "n_head": 10, "a_quant": "HalfHadamardFP4TrustQuantizer", "compile": true, "dataset": "c4", "dropout": 0, "n_layer": 10, "w_quant": "HalfHadamardFP4TrustQuantizer", "init_std": 0.02, "wa_dtype": "float32", "acc_steps": 1, "data_seed": 1337, "ema_decay": 0.95, "grad_clip": 1, "scheduler": "cos", "tokenizer": "gpt2", "batch_size": 64, "decay_type": "linear", "from_dense": false, "iterations": 76293, "vocab_size": 50304, "wa_horizon": 500, "world_size": 8, "auto_resume": true, "data_in_ram": false, "multiple_of": 256, "resume_from": null, "rmsnorm_eps": 1e-05, "wa_interval": 5, "datasets_dir": "./datasets/", "ema_interval": 10, "eval_batches": 32, "full_eval_at": [], "log_dynamics": false, "log_interval": 50, "warmup_steps": 7629, "weight_decay": 0.1, "config_format": "base", "cos_inf_steps": 0, "eval_interval": 200, "wandb_project": "llm-baselines", "parallel_block": false, "use_pretrained": "none", "weight_average": false, "eval_seq_prefix": "none", "experiment_name": null, "resume_from_swa": null, "sequence_length": 512, "wa_use_temp_dir": false, "wsd_fract_decay": 0.1, "ema_after_warmup": false, "wa_sweep_horizon": false, "wandb_run_prefix": "UNTIED-200M-HalfHadamardFP4TrustQuantizer@4:HalfHadamardFP4TrustQuantizer@4-c4", "max_num_wa_sweeps": 5, "mlp_dim_exp_factor": 1, "wsd_final_lr_scale": 0, "distributed_backend": "nccl", "dynamics_logger_cfg": "./src/logger/rotational_logger.yaml", "results_base_folder": "./exps", "latest_ckpt_interval": 1000, "permanent_ckpt_interval": 0, "exponential_moving_average": false}}]