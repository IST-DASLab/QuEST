{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/stabesh/micromamba/envs/quest/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.0000, 11.5285, -5.9167,  9.1315],\n",
      "        [-9.4046, -3.7989,  6.5704, -9.5838],\n",
      "        [-7.1115, 18.2400, -0.9039, -3.0564],\n",
      "        [14.4162, -2.6938, -5.0514,  7.3895]], requires_grad=True)\n",
      "tensor([[ 10.,  12.,  -6.,   9.],\n",
      "        [ -9.,  -4.,   7., -10.],\n",
      "        [ -7.,  18.,  -1.,  -3.],\n",
      "        [ 14.,  -3.,  -5.,   7.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def ste_round(x):\n",
    "    return (x.round() - x).detach() + x\n",
    "\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "x[0, 0] = 1\n",
    "x = x*10\n",
    "x = x.requires_grad_(True)\n",
    "print(x)\n",
    "y = ste_round(x)\n",
    "# y = torch.round(x)\n",
    "print(y)\n",
    "y.sum().backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "OPTIMAL_GAUSSIAN_SCALES = {\n",
    "    1: 0.7978845587140913,\n",
    "    2: 1.4935346200015913,\n",
    "    3: 2.051068354131873,\n",
    "    4: 2.513930578568423,\n",
    "    5: 2.9160938834961225,\n",
    "    6: 3.276597282593217,\n",
    "    7: 3.6010497188221655,\n",
    "    8: 3.884938678807525,\n",
    "}\n",
    "\n",
    "\n",
    "class SimplifiedHadamardClipQuantizer(nn.Module):\n",
    "    aux_matrix = hadamard_transform(\n",
    "        torch.eye(128, dtype=torch.bfloat16, device=\"cuda\"), scale=2 ** (-7 / 2)\n",
    "    )\n",
    "\n",
    "    def __init__(self, bits=4):\n",
    "        super().__init__()\n",
    "        self.bits = bits\n",
    "        self.n_levels = 2**bits\n",
    "        self.matrix = None\n",
    "\n",
    "    def forward1(self, x):\n",
    "        if self.matrix is None:\n",
    "            self.matrix = torch.block_diag(\n",
    "                *[self.aux_matrix.to(x.device).to(x.dtype)] * (x.shape[-1] // 128),\n",
    "            )\n",
    "\n",
    "        x_had = x @ self.matrix\n",
    "        with torch.no_grad():\n",
    "            scale = (\n",
    "                OPTIMAL_GAUSSIAN_SCALES[self.bits]\n",
    "                * torch.sqrt(torch.mean(x_had**2, dim=-1, keepdim=True))\n",
    "                + 1e-8\n",
    "            )\n",
    "            step = 2 * scale / (self.n_levels - 1)\n",
    "            x_clip = torch.clamp(x_had, -scale, scale)\n",
    "            xq = torch.round(x_clip / step + 1 / 2) * step - step / 2\n",
    "            mask = (torch.abs(x_had) <= scale).float()\n",
    "            xq = xq @ self.matrix.T\n",
    "\n",
    "        grad_flow_output = (x_had * mask) @ self.matrix.T\n",
    "\n",
    "        return grad_flow_output + (xq - grad_flow_output).detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def ste_round(x):\n",
    "        return (x.round() - x).detach() + x\n",
    "\n",
    "    def forward2(self, x):\n",
    "        if self.matrix is None:\n",
    "            self.matrix = torch.block_diag(\n",
    "                *[self.aux_matrix.to(x.device).to(x.dtype)] * (x.shape[-1] // 128),\n",
    "            )\n",
    "\n",
    "        x_had = x @ self.matrix\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scale = (\n",
    "                OPTIMAL_GAUSSIAN_SCALES[self.bits]\n",
    "                * torch.sqrt(torch.mean(x_had**2, dim=-1, keepdim=True))\n",
    "                + 1e-8\n",
    "            )\n",
    "            step = 2 * scale / (self.n_levels - 1)\n",
    "\n",
    "        x_clip = torch.clamp(x_had, -scale, scale)\n",
    "        xq = self.ste_round(x_clip / step + 1 / 2) * step - step / 2\n",
    "        xq = xq @ self.matrix.T\n",
    "\n",
    "        return xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dist='normal', shape=(32, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='normal', shape=(1, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='normal', shape=(64, 256)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='uniform', shape=(32, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='uniform', shape=(1, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='uniform', shape=(64, 256)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='positive_uniform', shape=(32, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='positive_uniform', shape=(1, 128)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n",
      "Testing dist='positive_uniform', shape=(64, 256)\n",
      "  Outputs match: True\n",
      "  Gradients match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_forward1_vs_forward2():\n",
    "\n",
    "    # We define multiple distributions and shapes to test\n",
    "    distributions = {\n",
    "        \"normal\": lambda shape: torch.randn(shape, device=\"cuda\"),\n",
    "        \"uniform\": lambda shape: torch.rand(shape, device=\"cuda\").mul(2).sub(1),  # in [-1, 1]\n",
    "        \"positive_uniform\": lambda shape: torch.rand(shape, device=\"cuda\"),       # in [0, 1]\n",
    "    }\n",
    "    shapes = [\n",
    "        (32, 128),  # typical 'batch x features'\n",
    "        (1, 128),   # single sample\n",
    "        (64, 256),  # bigger batch and features\n",
    "    ]\n",
    "\n",
    "    for dist_name, dist_func in distributions.items():\n",
    "        for shape in shapes:\n",
    "            q = SimplifiedHadamardClipQuantizer(bits=4).cuda()\n",
    "\n",
    "            torch.manual_seed(0)  # reset seed for reproducibility\n",
    "            print(f\"Testing dist='{dist_name}', shape={shape}\")\n",
    "\n",
    "            # Create input x\n",
    "            x_init = dist_func(shape).requires_grad_(True)\n",
    "\n",
    "            # forward1\n",
    "            x1 = x_init.clone().detach().requires_grad_(True)\n",
    "            out1 = q.forward1(x1)\n",
    "\n",
    "            # forward2\n",
    "            x2 = x_init.clone().detach().requires_grad_(True)\n",
    "            out2 = q.forward2(x2)\n",
    "\n",
    "            # Compare forward outputs\n",
    "            same_output = torch.allclose(out1, out2, atol=1e-6, rtol=1e-6)\n",
    "            print(f\"  Outputs match: {same_output}\")\n",
    "            if not same_output:\n",
    "                max_diff_output = (out1 - out2).abs().max().item()\n",
    "                print(f\"  Max diff in outputs: {max_diff_output:.3e}\")\n",
    "\n",
    "            # Compare backward gradients\n",
    "            out1.backward(torch.ones_like(out1))\n",
    "            out2.backward(torch.ones_like(out2))\n",
    "\n",
    "            same_grad = torch.allclose(x1.grad, x2.grad, atol=1e-6, rtol=1e-6)\n",
    "            print(f\"  Gradients match: {same_grad}\")\n",
    "            if not same_grad:\n",
    "                max_diff_grad = (x1.grad - x2.grad).abs().max().item()\n",
    "                print(f\"  Max diff in grads: {max_diff_grad:.3e}\")\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "test_forward1_vs_forward2()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
