{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc6613dce7f4ad6b558c592db75133d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd03506f6949debc2b7629f18eed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415d9d170d3e45b8b97a7805232105d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "summary.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/mloscratch/homes/panferov/schedules-and-scaling/hadamard-testing-800m'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"daslab-testing/hadamard-testing-800m\", local_dir=\"../hadamard-testing-800m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from optim.utils import load_checkpoint\n",
    "from models.utils import get_model\n",
    "\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "\n",
    "PATH = \"../hadamard-testing-800m\"\n",
    "with open(f\"{PATH}/summary.json\", \"r\") as f:\n",
    "    config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "from models.quantization.base_linear import OPTIMAL_GAUSSIAN_SCALES, HadamardTrustQuantizer, HalfHadamardTrustQuantizer\n",
    "\n",
    "\n",
    "def quantize_pack_hadamard_dense(x: torch.Tensor, quantizer: HadamardTrustQuantizer):\n",
    "    assert quantizer.centered\n",
    "    x_had = hadamard_transform(x.reshape(-1, 128), scale=2 ** (-7/2)).reshape(x.shape)\n",
    "    \n",
    "    std = torch.sqrt(torch.mean(x_had**2, dim=-1, keepdim=True)) + 1e-8\n",
    "    scale = OPTIMAL_GAUSSIAN_SCALES[quantizer.bits] * std\n",
    "\n",
    "    step = 2 * scale / (quantizer.n_levels - 1)\n",
    "    x_clip = torch.clamp(x_had, -scale, scale)\n",
    "    xq = torch.round((x_clip + scale) / step)\n",
    "\n",
    "    assert xq.min() >= 0 and xq.max() < quantizer.n_levels\n",
    "    return xq, scale, step\n",
    "    # ^ note: xq is in rotated space!\n",
    "\n",
    "def dequantize_dense(xq, scale, step):\n",
    "    return xq * step - scale\n",
    "\n",
    "weight = torch.rand(2, 128).cuda()\n",
    "quantizer = HadamardTrustQuantizer(bits=4)\n",
    "ref = quantizer(weight)\n",
    "xq, scale, step = quantize_pack_hadamard_dense(weight, quantizer)\n",
    "deq = dequantize_dense(xq, scale, step)\n",
    "\n",
    "torch.testing.assert_close(hadamard_transform(ref.reshape(-1, 128), scale=2 ** (-7/2)).reshape(ref.shape), deq, rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.quantization.base_linear import QuantizedLinear\n",
    "\n",
    "class Linear4bit(nn.Module):\n",
    "    def __init__(self, quantizer_linear):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert isinstance(quantizer_linear.weight_quantizer, HadamardTrustQuantizer)\n",
    "        assert isinstance(quantizer_linear.activation_quantizer, HadamardTrustQuantizer)\n",
    "        \n",
    "        self.activation_quantizer = quantizer_linear.activation_quantizer\n",
    "        \n",
    "        wq = dequantize_dense(*quantize_pack_hadamard_dense(quantizer_linear.weight, quantizer_linear.weight_quantizer))\n",
    "        self.register_buffer(\"wq\", wq)\n",
    "        self.bias = quantizer_linear.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = dequantize_dense(*quantize_pack_hadamard_dense(x, self.activation_quantizer))\n",
    "        return F.linear(x, self.wq, self.bias)\n",
    "\n",
    "\n",
    "def replace_linears(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, QuantizedLinear):\n",
    "            model._modules[name] = Linear4bit(module)\n",
    "        else:\n",
    "            replace_linears(module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDdp(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self._orig_mod = nn.ModuleDict({\n",
    "            \"module\": model,\n",
    "        })\n",
    "        \n",
    "class PseudoLoader:\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "model = PseudoDdp(get_model(DotDict(config['args'])))\n",
    "load_checkpoint(model, PseudoLoader(), PseudoLoader(), f\"{PATH}/main.pt\", \"cuda\")\n",
    "model = model.cuda()\n",
    "model = model._orig_mod[\"module\"]\n",
    "model = replace_linears(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Sign in to let us know how The Pizza Shack was?\n",
      "If you've been\n"
     ]
    }
   ],
   "source": [
    "def generate_text_greedily(model, tokenizer, prompt, max_length=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, get_logits=True)\n",
    "            logits = outputs['logits'][:, -1, :]\n",
    "        \n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text = generate_text_greedily(model, tokenizer, \"Hi!\", max_length=20)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822.083584\n"
     ]
    }
   ],
   "source": [
    "numel = 0\n",
    "for name, param in model.named_buffers():\n",
    "    numel += param.numel()\n",
    "    # print(name, param.numel())\n",
    "    \n",
    "print(numel/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
